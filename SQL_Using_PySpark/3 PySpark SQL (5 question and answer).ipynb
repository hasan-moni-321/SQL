{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"PySpark SQL Five Question\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing File Type and path for data train\n",
    "file_type = 'text'\n",
    "delimeter=','\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_type):\n",
    "    \"\"\"input type of file \"text\" or \"parquet\" and Return pyspark dataframe\"\"\"\n",
    "    if file_type ==\"text\": # use text as file type input\n",
    "        df = spark.read.option(\"header\", \"true\") \\\n",
    "                       .option(\"delimeter\",delimeter)\\\n",
    "                       .option(\"inferSchema\", \"true\") \\\n",
    "                       .csv(path)  #path file that you want import\n",
    "    else:  \n",
    "        df= spark.read.parquet(\"example.parquet\") #path file that you want import\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading worker dataset\n",
    "path=r'/home/hasan/DATA SET/PySpark SQL/Employee.csv'\n",
    "employee = load_data(file_type)\n",
    "\n",
    "# reading department dataset\n",
    "path = r'/home/hasan/DATA SET/PySpark SQL/department.csv'\n",
    "department = load_data(file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting to DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee = employee.withColumn(\"HireDate\", employee[\"HireDate\"].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+-----------------+-------+----------+------+----------+--------+\n",
      "|EmpCode|EmpFName|EmpLName|              Job|Manager|  HireDate|Salary|Commission|DEPTCODE|\n",
      "+-------+--------+--------+-----------------+-------+----------+------+----------+--------+\n",
      "|   9369|    TONY|   STARK|SOFTWARE ENGINEER|   7902|1980-12-17|  2800|         0|      20|\n",
      "|   9499|     TIM|   ADOLF|         SALESMAN|   7698|1981-02-20|  1600|       300|      30|\n",
      "|   9566|     KIM|  JARVIS|          MANAGER|   7839|1981-04-02|  3570|         0|      20|\n",
      "|   9654|     SAM|   MILES|         SALESMAN|   7698|1981-09-28|  1250|      1400|      30|\n",
      "|   9782|   KEVIN|    HILL|          MANAGER|   7839|1981-06-09|  2940|         0|      10|\n",
      "|   9788|  CONNIE|   SMITH|          ANALYST|   7566|1982-12-09|  3000|         0|      20|\n",
      "|   9839|  ALFRED| KINSLEY|        PRESIDENT|   7566|1981-11-17|  5000|         0|      10|\n",
      "|   9844|    PAUL| TIMOTHY|         SALESMAN|   7698|1981-09-08|  1500|         0|      30|\n",
      "|   9876|    JOHN|  ASGHAR|SOFTWARE ENGINEER|   7788|1983-01-12|  3100|         0|      20|\n",
      "|   9900|    ROSE| SUMMERS|   TECHNICAL LEAD|   7698|1981-12-03|  2950|         0|      20|\n",
      "|   9902|  ANDREW|FAULKNER|          ANALYST|   7566|1981-12-03|  3000|         0|      10|\n",
      "|   9934|   KAREN|MATTHEWS|SOFTWARE ENGINEER|   7782|1982-01-23|  3300|         0|      20|\n",
      "|   9591|   WENDY|   SHAWN|         SALESMAN|   7698|1981-02-22|   500|         0|      30|\n",
      "|   9698|   BELLA|    SWAN|          MANAGER|   7839|1981-05-01|  3420|         0|      30|\n",
      "|   9777|   MADII| HIMBURY|          ANALYST|   7839|1981-05-01|  2000|       200|      40|\n",
      "|   9860|  ATHENA|  WILSON|          ANALYST|   7839|1992-06-21|  7000|       100|      50|\n",
      "|   9861|JENNIFER|  HUETTE|          ANALYST|   7839|1996-07-01|  5000|       100|      50|\n",
      "+-------+--------+--------+-----------------+-------+----------+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EmpCode: integer (nullable = true)\n",
      " |-- EmpFName: string (nullable = true)\n",
      " |-- EmpLName: string (nullable = true)\n",
      " |-- Job: string (nullable = true)\n",
      " |-- Manager: integer (nullable = true)\n",
      " |-- HireDate: date (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Commission: integer (nullable = true)\n",
      " |-- DEPTCODE: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------+\n",
      "|DEPTCODE| DeptName|  LOCATION|\n",
      "+--------+---------+----------+\n",
      "|      10|  FINANCE| EDINBURGH|\n",
      "|      20| SOFTWARE|PADDINGTON|\n",
      "|      30|    SALES| MAIDSTONE|\n",
      "|      40|MARKETING|DARLINGTON|\n",
      "|      50|    ADMIN|BIRMINGHAM|\n",
      "+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEPTCODE: integer (nullable = true)\n",
      " |-- DeptName: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame as SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee.createOrReplaceTempView('employee_data')\n",
    "department.createOrReplaceTempView('department_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Question and Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+---------+----------+\n",
      "|EmpFName|EmpLName|DEPTCODE| DeptName|  LOCATION|\n",
      "+--------+--------+--------+---------+----------+\n",
      "|  ALFRED| KINSLEY|      10|  FINANCE| EDINBURGH|\n",
      "|  ANDREW|FAULKNER|      10|  FINANCE| EDINBURGH|\n",
      "|  ATHENA|  WILSON|      50|    ADMIN|BIRMINGHAM|\n",
      "|   BELLA|    SWAN|      30|    SALES| MAIDSTONE|\n",
      "|  CONNIE|   SMITH|      20| SOFTWARE|PADDINGTON|\n",
      "|JENNIFER|  HUETTE|      50|    ADMIN|BIRMINGHAM|\n",
      "|    JOHN|  ASGHAR|      20| SOFTWARE|PADDINGTON|\n",
      "|   KAREN|MATTHEWS|      20| SOFTWARE|PADDINGTON|\n",
      "|   KEVIN|    HILL|      10|  FINANCE| EDINBURGH|\n",
      "|     KIM|  JARVIS|      20| SOFTWARE|PADDINGTON|\n",
      "|   MADII| HIMBURY|      40|MARKETING|DARLINGTON|\n",
      "|    PAUL| TIMOTHY|      30|    SALES| MAIDSTONE|\n",
      "|    ROSE| SUMMERS|      20| SOFTWARE|PADDINGTON|\n",
      "|     SAM|   MILES|      30|    SALES| MAIDSTONE|\n",
      "|     TIM|   ADOLF|      30|    SALES| MAIDSTONE|\n",
      "|    TONY|   STARK|      20| SOFTWARE|PADDINGTON|\n",
      "|   WENDY|   SHAWN|      30|    SALES| MAIDSTONE|\n",
      "+--------+--------+--------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1 Create a query that displays EMPFNAME, EMPLNAME, DEPTCODE, DEPTNAME, LOCATION from EMPLOYEE, and \n",
    "#DEPARTMENT tables. Make sure the results are in the ascending order based on the EMPFNAME and LOCATION of the department.\n",
    "spark.sql(\" select E.EmpFName, E.EmpLName, E.DEPTCODE, D.DeptName, D.LOCATION from  employee_data E, department_data D where E.DEPTCODE=D.DEPTCODE order by EmpFName, LOCATION \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|EmpFName|Total_salary|\n",
      "+--------+------------+\n",
      "|    TONY|        2950|\n",
      "|    TONY|        5100|\n",
      "|    TONY|        3570|\n",
      "|    TONY|        2940|\n",
      "|    TONY|        2650|\n",
      "|    TONY|        3420|\n",
      "|    TONY|        3000|\n",
      "|    TONY|        1500|\n",
      "|    TONY|        2800|\n",
      "|    TONY|        5000|\n",
      "|    TONY|        1900|\n",
      "|    TONY|        3300|\n",
      "|    TONY|         500|\n",
      "|    TONY|        2200|\n",
      "|    TONY|        3100|\n",
      "|    TONY|        7100|\n",
      "|    TONY|        3000|\n",
      "|     TIM|        2950|\n",
      "|     TIM|        5100|\n",
      "|     TIM|        3570|\n",
      "+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2 Display EMPFNAME and “TOTAL SALARY” for each employee\n",
    "spark.sql(\" select EmpFName, result1.Total_salary from employee_data, (select sum(Salary+Commission) as Total_salary from employee_data group by EmpCode) result1 \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Salary|\n",
      "+------+\n",
      "|  7000|\n",
      "|  5000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3 Display MAX and 2nd MAX SALARY from the EMPLOYEE table. \n",
    "spark.sql(\" select Salary from employee_data order by Salary desc limit 2 \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Second_Max|\n",
      "+----------+\n",
      "|      5000|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 Display 2nd MAX SALARY from the EMPLOYEE table.\n",
    "spark.sql(\" select max(Salary) as Second_Max from employee_data where Salary not in (select max(Salary) from employee_data) \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|scalarsubquery()|\n",
      "+----------------+\n",
      "|            5000|\n",
      "|            7000|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3 Display MAX and 2nd MAX SALARY from the EMPLOYEE table.\n",
    "spark.sql(\" select \\\n",
    "          (select max(Salary) as MaxSalary from employee_data) \\\n",
    "          union \\\n",
    "          (select max(Salary) as SecondMax from employee_data where Salary not in (select max(Salary) from employee_data)) \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Total_Salary|\n",
      "+------------+\n",
      "|        3000|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4 Display the TOTAL SALARY drawn by an analyst working in dept no 20\n",
    "spark.sql(\" select sum(Salary+Commission) as Total_Salary from employee_data where Job='ANALYST'  and DEPTCODE=20 \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|average|minimum|maximum|\n",
      "+-------+-------+-------+\n",
      "| 4000.0|   2000|   7000|\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5 Compute average, minimum and maximum salaries of the group of employees having the job of ANALYST.\n",
    "spark.sql(\" select avg(Salary) as average, min(Salary) as minimum, max(Salary) as maximum from employee_data where Job='ANALYST' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
